{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing the NVFP4 Recipe From Scratch: A Developer's Tutorial",
        "",
        "This tutorial deconstructs the core algorithms from PR #2177 to teach you how to implement them conceptually. We will build Python/PyTorch reference functions that mirror the logic of the new C++/CUDA kernels.",
        "",
        "Our goal is to implement these key components:",
        "",
        "1.  **Core 1D Block Quantization**: The fundamental scaling and casting logic for 1x16 blocks.",
        "2.  **2D Block Quantization**: An extension for quantizing 16x16 blocks, ideal for weights.",
        "3.  **Random Hadamard Transform (RHT)**: The pre-quantization step to improve accuracy.",
        "4.  **The Fused Operation**: Combining everything to produce the final `NVFP4Tensor` components.",
        "",
        "We will focus on the *algorithmic logic*, not CUDA-level performance optimizations.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch",
        "import math",
        "",
        "# For reproducibility",
        "torch.manual_seed(0)",
        "torch.cuda.manual_seed(0)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Understanding the Target - The NVFP4 E2M1 Format",
        "",
        "Before we can quantize, we need to know what we're converting *to*. NVFP4 in this PR uses the `E2M1` format (2 exponent bits, 1 mantissa bit). It's a 4-bit floating-point number. We can represent all possible 16 values in a lookup table (LUT). This helps us simulate the casting process.",
        "",
        "The C++ code uses native `__nv_fp4_e2m1` types, but this LUT is perfect for a Python reference.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The 16 possible values for an E2M1 FP4 number.",
        "# Index corresponds to the 4-bit integer representation.",
        "FP4_E2M1_LUT = torch.tensor([",
        "    # Positive values (first bit 0)",
        "    0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0,",
        "    # Negative values (first bit 1)",
        "    -0.0, -0.5, -1.0, -1.5, -2.0, -3.0, -4.0, -6.0,",
        "], dtype=torch.float32)",
        "",
        "# The maximum absolute value for E2M1 is 6.0. This is a critical constant.",
        "FP4_E2M1_MAX_VAL = 6.0",
        "",
        "def find_closest_fp4_val(value):",
        "    \"\"\"Simulates casting a float to the nearest FP4 value.\"\"\"",
        "    # Find the value in our LUT that is closest to the input value.",
        "    # The index of this closest value is our 4-bit representation.",
        "    return torch.argmin(torch.abs(value - FP4_E2M1_LUT.to(value.device)))",
        "",
        "print(f\"FP4 E2M1 Lookup Table:\\n{FP4_E2M1_LUT}\")",
        "print(f\"\\nExample: Casting 2.9 to FP4 -> finds value {FP4_E2M1_LUT[find_closest_fp4_val(torch.tensor(2.9))]}\")",
        "print(f\"Example: Casting -4.2 to FP4 -> finds value {FP4_E2M1_LUT[find_closest_fp4_val(torch.tensor(-4.2))]}\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Implementing 1D Block Quantization",
        "",
        "This is the core logic. For each 1D block of 16 elements in a tensor row, we perform these steps. This logic is what the reference implementation `quantize_nvfp4_1d` in `test_cast_nvfp4_transpose.cu` performs.",
        "",
        "1.  Find the absolute maximum value (`amax`) in the 16-element block.",
        "2.  Calculate a `scaling_factor` for this block. The formula is `amax / FP4_E2M1_MAX_VAL`.",
        "3.  **Scale** the original 16 values by dividing by the `scaling_factor`.",
        "4.  **Cast** the scaled values to the nearest FP4 value.",
        "5.  Store the resulting 16 4-bit integers and the single `scaling_factor`.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quantize_1d_block_reference(hp_tensor: torch.Tensor):",
        "    \"\"\"",
        "    Reference implementation for 1D block quantization (1x16 blocks).",
        "    \"\"\"",
        "    assert hp_tensor.dim() == 2, \"Input must be a 2D tensor\"",
        "    rows, cols = hp_tensor.shape",
        "    assert cols % 16 == 0, \"Columns must be divisible by 16\"",
        "",
        "    # Outputs",
        "    num_scale_blocks = cols // 16",
        "    quantized_data = torch.zeros(rows, cols, dtype=torch.int8, device=hp_tensor.device)",
        "    scaling_factors = torch.zeros(rows, num_scale_blocks, dtype=hp_tensor.dtype, device=hp_tensor.device)",
        "",
        "    for i in range(rows):",
        "        for j in range(num_scale_blocks):",
        "            # 1. Get the 1x16 block",
        "            start_col, end_col = j * 16, (j + 1) * 16",
        "            block = hp_tensor[i, start_col:end_col]",
        "",
        "            # 2. Find amax",
        "            block_amax = torch.max(torch.abs(block))",
        "            if block_amax == 0: # Handle all-zero blocks",
        "                scaling_factors[i, j] = 0.0",
        "                # Quantized data is already 0",
        "                continue",
        "",
        "            # 3. Calculate scaling factor",
        "            scaling_factor = block_amax / FP4_E2M1_MAX_VAL",
        "            scaling_factors[i, j] = scaling_factor",
        "",
        "            # 4. Scale the block",
        "            scaled_block = block / scaling_factor",
        "",
        "            # 5. Cast to FP4 (by finding closest value in LUT)",
        "            for k in range(16):",
        "                quantized_data[i, start_col + k] = find_closest_fp4_val(scaled_block[k])",
        "",
        "    return quantized_data, scaling_factors",
        "",
        "# --- Test it ---",
        "sample_tensor = torch.randn((2, 32), dtype=torch.bfloat16, device='cuda')",
        "q_data_1d, scales_1d = quantize_1d_block_reference(sample_tensor)",
        "",
        "print(\"--- 1D Quantization Example ---\")",
        "print(f\"Original Tensor Shape: {sample_tensor.shape}\")",
        "print(f\"Quantized Data Shape: {q_data_1d.shape} (stores 4-bit integer indices)\")",
        "print(f\"Scaling Factors Shape: {scales_1d.shape}\")",
        "print(\"\\nFirst row's scaling factors:\")",
        "print(scales_1d[0])",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implementing 2D Block Quantization",
        "",
        "The PR enables 2D quantization for weights. The logic is similar, but the block size is now 16x16. There is only **one scaling factor for the entire 256-element block**. This is implemented in the reference function `quantize_nvfp4_2d` in `test_cast_nvfp4_transpose.cu`.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quantize_2d_block_reference(hp_tensor: torch.Tensor):",
        "    \"\"\"",
        "    Reference implementation for 2D block quantization (16x16 blocks).",
        "    \"\"\"",
        "    assert hp_tensor.dim() == 2, \"Input must be a 2D tensor\"",
        "    rows, cols = hp_tensor.shape",
        "    assert rows % 16 == 0 and cols % 16 == 0, \"Dimensions must be divisible by 16\"",
        "",
        "    # Outputs",
        "    num_blocks_y, num_blocks_x = rows // 16, cols // 16",
        "    quantized_data = torch.zeros_like(hp_tensor, dtype=torch.int8)",
        "    scaling_factors = torch.zeros(num_blocks_y, num_blocks_x, dtype=hp_tensor.dtype, device=hp_tensor.device)",
        "",
        "    for i in range(num_blocks_y):",
        "        for j in range(num_blocks_x):",
        "            # 1. Get the 16x16 block",
        "            start_row, end_row = i * 16, (i + 1) * 16",
        "            start_col, end_col = j * 16, (j + 1) * 16",
        "            block = hp_tensor[start_row:end_row, start_col:end_col]",
        "",
        "            # 2. Find amax for the entire 16x16 block",
        "            block_amax = torch.max(torch.abs(block))",
        "            if block_amax == 0:",
        "                scaling_factors[i, j] = 0.0",
        "                continue",
        "",
        "            # 3. Calculate scaling factor",
        "            scaling_factor = block_amax / FP4_E2M1_MAX_VAL",
        "            scaling_factors[i, j] = scaling_factor",
        "",
        "            # 4. Scale the block",
        "            scaled_block = block / scaling_factor",
        "",
        "            # 5. Cast to FP4",
        "            # (Vectorized version for simplicity)",
        "            quantized_block = torch.zeros_like(scaled_block, dtype=torch.int8)",
        "            for y in range(16):",
        "                for x in range(16):",
        "                    quantized_block[y, x] = find_closest_fp4_val(scaled_block[y, x])",
        "            quantized_data[start_row:end_row, start_col:end_col] = quantized_block",
        "",
        "    return quantized_data, scaling_factors",
        "",
        "",
        "# --- Test it ---",
        "sample_tensor_2d = torch.randn((32, 64), dtype=torch.bfloat16, device='cuda')",
        "q_data_2d, scales_2d = quantize_2d_block_reference(sample_tensor_2d)",
        "",
        "print(\"--- 2D Quantization Example ---\")",
        "print(f\"Original Tensor Shape: {sample_tensor_2d.shape}\")",
        "print(f\"Quantized Data Shape: {q_data_2d.shape}\")",
        "print(f\"Scaling Factors Shape: {scales_2d.shape} (2x4 blocks of 16x16)\")",
        "print(\"\\nScaling factors for all 16x16 blocks:\")",
        "print(scales_2d)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Implementing Random Hadamard Transform (RHT)",
        "",
        "RHT is a pre-processing step applied to activations before quantization. It's a matrix multiplication with a special \"Hadamard\" matrix. The goal is to distribute the information across the vector, making quantization less lossy. The PR adds highly optimized kernels for this (`hadamard_transform_cast_fusion.cu`).",
        "",
        "Our reference will build the matrix and apply it block-wise.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hadamard_matrix(size, device):",
        "    \"\"\"Constructs a Hadamard matrix of a power-of-two size.\"\"\"",
        "    if size == 1:",
        "        return torch.ones((1, 1), device=device)",
        "    h_prev = get_hadamard_matrix(size // 2, device)",
        "    h_next = torch.cat([",
        "        torch.cat([h_prev, h_prev], dim=1),",
        "        torch.cat([h_prev, -h_prev], dim=1),",
        "    ], dim=0)",
        "    return h_next",
        "",
        "def random_hadamard_transform_reference(hp_tensor: torch.Tensor):",
        "    \"\"\"Applies a 16x16 RHT to the tensor block-wise.\"\"\"",
        "    rows, cols = hp_tensor.shape",
        "    assert cols % 16 == 0, \"Columns must be divisible by 16\"",
        "",
        "    # The transform matrix includes normalization",
        "    h_matrix = get_hadamard_matrix(16, hp_tensor.device).to(hp_tensor.dtype)",
        "    h_matrix *= (1.0 / math.sqrt(16))",
        "",
        "    transformed_tensor = torch.zeros_like(hp_tensor)",
        "",
        "    for i in range(rows):",
        "        for j in range(cols // 16):",
        "            start_col, end_col = j * 16, (j + 1) * 16",
        "            block = hp_tensor[i, start_col:end_col]",
        "            # Apply the transform: block @ H",
        "            transformed_block = torch.matmul(block, h_matrix)",
        "            transformed_tensor[i, start_col:end_col] = transformed_block",
        "",
        "    return transformed_tensor",
        "",
        "# --- Test it ---",
        "sample_tensor_rht = torch.randn((1, 32), dtype=torch.bfloat16, device='cuda')",
        "transformed_tensor = random_hadamard_transform_reference(sample_tensor_rht)",
        "",
        "print(\"--- RHT Example ---\")",
        "print(\"Original first 16 values:\\n\", sample_tensor_rht[0, :16])",
        "print(\"\\nTransformed first 16 values:\\n\", transformed_tensor[0, :16])",
        "print(f\"Shape remains the same: {transformed_tensor.shape}\")",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: The Fused Operation - Putting It All Together",
        "",
        "The true power of the PR is fusing all these steps into a single, efficient CUDA kernel. The kernel performs:",
        "`Cast -> RHT (optional) -> Quantize -> Transpose -> Quantize (again for transposed layout)`",
        "",
        "This avoids materializing intermediate tensors in memory and is much faster. Let's create a Python function that orchestrates our reference components to simulate this entire pipeline. This mimics the `compute_ref` function in `test_cast_nvfp4_transpose.cu`.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nvfp4_recipe_reference(",
        "    hp_tensor: torch.Tensor,",
        "    use_rht: bool,",
        "    use_2d_quant_for_weights: bool # In TE, this only applies to weights, but we simulate it here",
        "):",
        "    \"\"\"",
        "    Simulates the full, fused quantization pipeline.",
        "    \"\"\"",
        "    # --- Process the input for row-wise (activation) usage ---",
        "    processed_tensor = random_hadamard_transform_reference(hp_tensor) if use_rht else hp_tensor",
        "    # Always use 1D quantization for activations/row-wise data",
        "    q_data, scales = quantize_1d_block_reference(processed_tensor)",
        "",
        "    # --- Process the input for column-wise (weight) usage ---",
        "    hp_tensor_t = hp_tensor.T.contiguous()",
        "    if use_2d_quant_for_weights:",
        "        # NOTE: Real implementation pads to 16x16 blocks. We'll assume divisible dimensions.",
        "        q_data_t, scales_t = quantize_2d_block_reference(hp_tensor_t)",
        "    else:",
        "        q_data_t, scales_t = quantize_1d_block_reference(hp_tensor_t)",
        "",
        "    print(\"Simulated fused operation successful!\")",
        "    return q_data, scales, q_data_t, scales_t",
        "",
        "# --- Test it with a realistic shape ---",
        "activation_tensor = torch.randn((128, 2048), dtype=torch.bfloat16, device='cuda')",
        "",
        "q_activation, scales_activation, q_weight, scales_weight = nvfp4_recipe_reference(",
        "    activation_tensor,",
        "    use_rht=True,",
        "    use_2d_quant_for_weights=True",
        ")",
        "",
        "print(\"\\n--- Outputs of the Fused Pipeline ---\")",
        "print(f\"Quantized Activation Shape: {q_activation.shape}\")",
        "print(f\"Activation Scales Shape: {scales_activation.shape}\")",
        "print(f\"Quantized Weight (Transposed) Shape: {q_weight.shape}\")",
        "print(f\"Weight Scales (Transposed) Shape: {scales_weight.shape}\")",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: The `NVFP4Tensor` Data Structure",
        "",
        "Finally, why does the PR introduce a new `NVFP4Tensor` class in Python?",
        "",
        "Because the results of the fused operation (`q_data`, `scales`, `q_data_t`, `scales_t`) all belong together. They represent a single high-precision tensor in its quantized form. The `NVFP4Tensor` acts as a container for all these components.",
        "",
        "When a TE layer needs the tensor for a forward pass GEMM (activations), it uses `q_data` and `scales`. When it needs the tensor for a wgrad GEMM (weights), it uses `q_data_t` and `scales_t`. This avoids costly re-quantization or transposing of packed 4-bit data on the fly.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass",
        "",
        "@dataclass",
        "class NVFP4TensorReference:",
        "    \"\"\"A Python dataclass to represent the real NVFP4Tensor structure.\"\"\"",
        "    _rowwise_data: torch.Tensor",
        "    _rowwise_scale_inv: torch.Tensor",
        "    _columnwise_data: torch.Tensor",
        "    _columnwise_scale_inv: torch.Tensor",
        "    original_shape: tuple",
        "",
        "# Let's package our results into this structure",
        "nvfp4_tensor_ref = NVFP4TensorReference(",
        "    _rowwise_data=q_activation,",
        "    _rowwise_scale_inv=scales_activation,",
        "    _columnwise_data=q_weight,",
        "    _columnwise_scale_inv=scales_weight,",
        "    original_shape=activation_tensor.shape",
        ")",
        "",
        "print(\"Representation of a complete NVFP4Tensor object:\")",
        "print(nvfp4_tensor_ref)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion",
        "",
        "You have now implemented the core algorithmic building blocks of the NVFP4 recipe from scratch.",
        "",
        "You've learned that the implementation is not just a simple cast, but a sophisticated, fused pipeline that involves:",
        "1.  **Block-based Scaling**: Calculating per-block scaling factors (either 1D or 2D).",
        "2.  **Optional Pre-processing (RHT)**: Applying a mathematical transform to improve numerical stability.",
        "3.  **Fused Operations**: Performing quantization and transposition in a single step to generate layouts for both forward and backward passes efficiently.",
        "4.  **A Specialized Data Structure**: Using `NVFP4Tensor` to hold all the necessary components (data, scales, transposed versions) together.",
        "",
        "The actual C++/CUDA code in the PR takes these exact algorithms and implements them with extreme performance optimizations, using techniques like shared memory, tensor core instructions, and careful data movement to make 4-bit training feasible at scale.",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Lessons: Implementing the NVFP4 Recipe",
        "",
        "Welcome to the advanced implementation tutorial for the NVFP4 recipe. In the previous session, we built high-level Python models of the core algorithms. Now, we will dissect the engineering principles and low-level details from the PR to understand how this is implemented for maximum performance on a GPU.",
        "",
        "### Learning Path:",
        "*   **Lesson 1: The \"Why\" of Fused Kernels** - Why not just call the Python functions in sequence?",
        "*   **Lesson 2: Anatomy of the CUDA Kernel** - A conceptual breakdown of the C++ `block_scaled_1d_cast_transpose_kernel`.",
        "*   **Lesson 3: The Nuances of Two-Level Scaling** - Understanding the global (`S_enc`) and local (`S_dec_b`) scaling factors.",
        "*   **Lesson 4: Distributed Training & Quantized All-Gather** - How to handle custom data types in a multi-GPU setting.",
        "*   **Lesson 5: The Python API Glue** - How the `NVFP4Quantizer` class orchestrates everything.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch",
        "import math",
        "",
        "# For reproducibility",
        "torch.manual_seed(0)",
        "torch.cuda.manual_seed(0)",
        "",
        "# Constants from the previous lesson",
        "FP4_E2M1_MAX_VAL = 6.0",
        "# A new constant from the PR: the max value of an FP8 E4M3 number, used for scaling factors.",
        "FP8_E4M3_MAX_VAL = 448.0",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 1: The \"Why\" of Fused Kernels - The Memory Bottleneck",
        "",
        "In our previous tutorial, we implemented each step (RHT, Quantize, Transpose) as a separate Python function. On a real GPU, this would be incredibly inefficient. Why? **Memory Bandwidth**.",
        "",
        "A GPU is fastest when it's doing math (computing). It's relatively slow when it's moving data between its main memory (HBM) and its compute cores. Operations like ours are often **memory-bound**, meaning the GPU spends more time waiting for data than computing on it.",
        "",
        "Consider the \"naive\" approach:",
        "1.  `hp_tensor` is in Global Memory.",
        "2.  **Kernel 1 (RHT)**: Load `hp_tensor`, compute RHT, write `rht_tensor` back to Global Memory.",
        "3.  **Kernel 2 (Amax)**: Load `rht_tensor`, compute amax, write `amax_tensor` back to Global Memory.",
        "4.  **Kernel 3 (Quantize)**: Load `rht_tensor` and `amax_tensor`, compute scales and quantized data, write `q_tensor` and `scales_tensor` to Global Memory.",
        "5.  ...and so on for the transpose.",
        "",
        "This involves multiple round-trips to slow global memory. A **fused kernel**, like the one in this PR (`quantize_transpose_vector_blockwise_fp4.cu`), does all of this in a single trip.",
        "",
        "### The Fused Kernel Strategy:",
        "1.  **Launch ONE Kernel.**",
        "2.  Threads load a small tile of the `hp_tensor` from Global Memory into ultra-fast **Shared Memory**.",
        "3.  Perform all operations (RHT, amax reduction, scaling, casting) directly on the data in Shared Memory.",
        "4.  Write the final, tiny outputs (`q_tensor` tile, `scales_tensor` tile) back to Global Memory.",
        "",
        "This minimizes global memory traffic and maximizes computation, leading to massive speedups. The entire PR is built around this principle.",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 2: Anatomy of a Fused CUDA Kernel",
        "",
        "Let's write a \"pseudo-code\" walkthrough of the main kernel. We can't run CUDA C++ here, but we can model its logic and structure in Python comments to understand how it works. We'll focus on the `block_scaled_1d_cast_transpose_kernel` logic from the new C++ tests.",
        "",
        "A CUDA kernel is executed by a grid of *thread blocks*. Each block is responsible for processing one \"tile\" of the input data. Inside a block, threads cooperate using **Shared Memory**.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conceptual_fused_kernel(hp_tensor):",
        "    \"\"\"A Python simulation of the fused kernel's logic for a single 16x16 tile.\"\"\"",
        "    # --- Kernel Launch Setup (Done by the CUDA runtime) ---",
        "    # Imagine this function is ONE thread block, given an index (blockIdx.x, blockIdx.y)",
        "    # to identify which 16x16 tile of the hp_tensor it should process.",
        "    # Let's assume this block is responsible for the tile starting at (0, 0).",
        "    TILE_DIM = 16",
        "    block_start_row, block_start_col = 0, 0",
        "",
        "    # --- Inside the Kernel (Execution on GPU) ---",
        "",
        "    # 1. Cooperative Loading into Shared Memory",
        "    # Each of the 256 threads in the block loads one element from global HBM",
        "    # into the fast, on-chip shared memory scratchpad.",
        "    shared_mem_tile = hp_tensor[",
        "        block_start_row : block_start_row + TILE_DIM,",
        "        block_start_col : block_start_col + TILE_DIM",
        "    ].clone() # .clone() simulates the copy to a new memory space.",
        "    # In CUDA, a `__syncthreads()` barrier would wait for all loads to complete.",
        "",
        "    # 2. On-Chip AMAX Reduction (Row-wise)",
        "    # The threads now work on the fast shared memory tile.",
        "    # They cooperatively find the amax for each of the 16 rows in the tile.",
        "    row_amaxes = torch.max(torch.abs(shared_mem_tile), dim=1).values",
        "    # This is a simplified view. In CUDA, this is a multi-step reduction using",
        "    # warp-level primitives (`__shfl_down_sync`) and another `__syncthreads()`.",
        "",
        "    # 3. Calculate Row-wise Scaling Factors",
        "    row_scales = row_amaxes / FP4_E2M1_MAX_VAL",
        "    # Handle division by zero for all-zero rows",
        "    row_scales[row_scales == 0] = 1.0",
        "",
        "    # 4. Scale and Cast (Row-wise)",
        "    # Each thread scales its value and simulates the cast.",
        "    # The actual CUDA kernel uses a PTX instruction like `cvt.rn.satfinite.e2m1x2.f32`",
        "    # which converts two FP32 numbers to two packed FP4 numbers in one go.",
        "    scaled_tile = shared_mem_tile / row_scales.unsqueeze(1)",
        "    quantized_tile = torch.round(scaled_tile).clamp(-FP4_E2M1_MAX_VAL, FP4_E2M1_MAX_VAL) # Simplified cast logic",
        "",
        "    # 5. On-Chip Transposition",
        "    # Threads cooperatively write to a second shared memory buffer in a transposed pattern.",
        "    transposed_shared_mem_tile = shared_mem_tile.T.contiguous()",
        "    # `__syncthreads()` ensures the transpose is complete.",
        "",
        "    # 6. AMAX, Scale, and Cast (Column-wise / Transposed)",
        "    # The process is repeated on the transposed tile to get the column-wise outputs.",
        "    col_amaxes = torch.max(torch.abs(transposed_shared_mem_tile), dim=1).values",
        "    col_scales = col_amaxes / FP4_E2M1_MAX_VAL",
        "    col_scales[col_scales == 0] = 1.0",
        "    scaled_transposed_tile = transposed_shared_mem_tile / col_scales.unsqueeze(1)",
        "    quantized_transposed_tile = torch.round(scaled_transposed_tile).clamp(-FP4_E2M1_MAX_VAL, FP4_E2M1_MAX_VAL)",
        "",
        "    # 7. Write Final Results to Global Memory",
        "    # The threads write their final results from shared memory back to the final output tensors in HBM.",
        "    # This is the only other time they touch global memory.",
        "    print(\"Conceptual kernel finished processing one tile.\")",
        "    return quantized_tile, row_scales, quantized_transposed_tile, col_scales",
        "",
        "# --- Run the conceptual model ---",
        "sample_tile = torch.randn((16, 16), dtype=torch.float32, device='cuda')",
        "q_data, scales, q_data_t, scales_t = conceptual_fused_kernel(sample_tile)",
        "",
        "print(f\"\\nRow-wise quantized data shape: {q_data.shape}\")",
        "print(f\"Row-wise scales shape: {scales.shape} (One scale per row in the tile)\")",
        "print(f\"Column-wise quantized data shape: {q_data_t.shape}\")",
        "print(f\"Column-wise scales shape: {scales_t.shape} (One scale per column in the tile)\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 3: The Nuances of Two-Level Scaling",
        "",
        "The previous lessons used a simplified scaling formula: `scale = amax / 6.0`. The actual implementation in the PR is more sophisticated, as seen in the C++ function `compute_global_encode_scaling_factor_FP4`. It uses a **two-level scaling system**.",
        "",
        "1.  **Global Per-Tensor Scale (`S_enc`)**: A single FP32 scale factor is computed for the *entire tensor*. Its job is to map the tensor's global amax into a range that is friendly to FP8-E4M3, the format used for the *scaling factors themselves*.",
        "",
        "2.  **Local Per-Block Scale (`S_dec_b`)**: This is the scale we've been calculating (`block_amax / 6.0`). It handles local variations.",
        "",
        "**The final scaling factor stored in memory is `S_final = S_dec_b * S_enc`**.",
        "",
        "Why do this? It improves numerical precision. By pre-scaling the entire tensor with `S_enc`, we ensure that the per-block `S_dec_b` values can be accurately represented by the FP8-E4M3 format.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def two_level_scaling_reference(hp_tensor: torch.Tensor):",
        "    \"\"\"Reference implementation for the two-level scaling logic.\"\"\"",
        "    # -- Level 1: Global Scaling --",
        "    global_amax = torch.max(torch.abs(hp_tensor))",
        "",
        "    # This formula is a direct translation of the C++ `compute_global_encode_scaling_factor_FP4`",
        "    # It maps the global amax to the dynamic range of FP8 * FP4",
        "    if global_amax == 0.0:",
        "        S_enc = 1.0",
        "    else:",
        "        S_enc = (FP8_E4M3_MAX_VAL * FP4_E2M1_MAX_VAL) / global_amax",
        "        S_enc = min(S_enc, torch.finfo(torch.float32).max) # Clamp to max float32",
        "",
        "    # -- Level 2: Local Scaling (within a 1D block) --",
        "    rows, cols = hp_tensor.shape",
        "    num_scale_blocks = cols // 16",
        "    final_scales = torch.zeros(rows, num_scale_blocks, dtype=torch.float32, device=hp_tensor.device)",
        "",
        "    for i in range(rows):",
        "        for j in range(num_scale_blocks):",
        "            block = hp_tensor[i, j*16:(j+1)*16]",
        "            block_amax = torch.max(torch.abs(block))",
        "",
        "            # Calculate the local decoding scale",
        "            S_dec_b = block_amax / FP4_E2M1_MAX_VAL",
        "",
        "            # Combine with global encoding scale to get the final scale",
        "            S_final = S_dec_b * S_enc",
        "",
        "            # The final scale is then cast to FP8 E4M3 for storage.",
        "            # We will just store it as float32 for this reference.",
        "            final_scales[i, j] = S_final",
        "",
        "    print(f\"Global Amax: {global_amax:.4f}, S_enc (Global Scale): {S_enc:.4f}\")",
        "    return final_scales",
        "",
        "# --- Test the two-level scaling ---",
        "sample_tensor = torch.randn((2, 32), device='cuda') * 10 # Scale up to see a more interesting amax",
        "final_scaling_factors = two_level_scaling_reference(sample_tensor)",
        "print(\"\\nFinal (two-level) scaling factors for the first row:\")",
        "print(final_scaling_factors[0])",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 4: Distributed Training & Quantized All-Gather",
        "",
        "Making a new feature work on one GPU is only half the battle. For large models, it must work with tensor parallelism across multiple GPUs. This PR adds a custom `_all_gather_nvfp4` function in `transformer_engine/pytorch/distributed.py`.",
        "",
        "**The Problem**: You can't just call `torch.distributed.all_gather` on an `NVFP4Tensor` object. The All-Gather operation only works on single, contiguous `torch.Tensor`s.",
        "",
        "**The Solution**:",
        "1.  Deconstruct the `NVFP4Tensor` on each GPU into its constituent `torch.Tensor` components (e.g., `_rowwise_data`, `_rowwise_scale_inv`).",
        "2.  Perform a separate `all_gather` operation on each component tensor.",
        "3.  Reconstruct a new, larger `NVFP4Tensor` on each GPU from the gathered components.",
        "",
        "**A New Problem (The \"Interleave\" Issue)**: When you gather a *transposed* tensor (like `_columnwise_data`) along the batch dimension, the data from different GPUs gets interleaved incorrectly.",
        "",
        "Imagine 2 GPUs. GPU0 has `[A0, B0]` and GPU1 has `[A1, B1]`. After gathering, the memory layout isn't `[A0, B0, A1, B1]`. It becomes something like `[A0, A1, B0, B1]`.",
        "",
        "To fix this, the PR adds a `swap_first_dims` operation. Let's simulate this.",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_distributed_gather_and_fix():",
        "    world_size = 4 # Simulate 4 GPUs",
        "    local_dim0, local_dim1 = 2, 8",
        "",
        "    # Create dummy transposed data on each GPU",
        "    gpu_data = [torch.arange(local_dim0 * local_dim1, dtype=torch.float32).reshape(local_dim0, local_dim1) + (i*100) for i in range(world_size)]",
        "    print(f\"--- Data on GPU 0 (Transposed Layout) ---\\n{gpu_data[0]}\")",
        "",
        "    # Simulate `all_gather` on the first dimension. This creates the interleaved result.",
        "    interleaved_data = torch.cat(gpu_data, dim=0)",
        "    print(f\"\\n--- Interleaved Data After All-Gather ---\\n{interleaved_data}\")",
        "",
        "    # The `swap_first_dims` logic to fix the layout",
        "    # This is what `tex.swap_first_dims` in the PR does in a highly optimized way.",
        "    total_dim0 = interleaved_data.shape[0]",
        "    fixed_data = interleaved_data.reshape(world_size, total_dim0 // world_size, -1).transpose(0, 1).reshape(total_dim0, -1)",
        "",
        "    print(f\"\\n--- Data After `swap_first_dims` Fix ---\\n{fixed_data}\")",
        "",
        "simulate_distributed_gather_and_fix()",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 5: The Python API Glue - `NVFP4Quantizer`",
        "",
        "The `NVFP4Quantizer` class in `transformer_engine/pytorch/tensor/nvfp4_tensor.py` is the high-level orchestrator. It's the bridge between the Python world and the C++/CUDA backend.",
        "",
        "Let's break down its key responsibilities based on the PR:",
        "",
        "1.  **Configuration (`__init__`)**: It reads the `Recipe` object and stores flags like `with_rht`, `stochastic_rounding`, and `with_2d_quantization`. It also pre-builds the RHT matrix if needed.",
        "",
        "2.  **State Management**: It holds stateful information. For example, it generates and stores the random sign mask for the RHT matrix.",
        "",
        "3.  **Backend Invocation (`quantize`)**: This is the main method. It takes a high-precision `torch.Tensor` as input.",
        "    *   It checks the tensor shape and properties.",
        "    *   It packages all the configuration flags and tensor pointers into a C-compatible structure (`QuantizationConfigWrapper`).",
        "    *   It calls the core C++ function (e.g., `tex.quantize_fp4`) through the Pybind11 bridge. This is the function that launches the fused CUDA kernel we discussed in Lesson 2.",
        "",
        "4.  **Object Creation**: The C++ function returns raw tensor data. The `NVFP4Quantizer` takes this raw data and uses it to construct and return a proper, user-friendly `NVFP4Tensor` Python object.",
        "",
        "This class design cleanly separates the high-level configuration and object management in Python from the low-level, high-performance computations in C++/CUDA.",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grand Conclusion",
        "",
        "You have now journeyed from a high-level user of the NVFP4 recipe to understanding the deepest implementation details. You've learned:",
        "",
        "-   **Performance is King**: Fused kernels are essential to overcome memory bandwidth limitations, which is the primary motivation for the C++/CUDA implementation.",
        "-   **CUDA Programming Patterns**: Thread blocks, shared memory, and cooperative execution are the tools used to build these fused kernels.",
        "-   **Numerical Precision Matters**: The two-level scaling system is a clever trick to maintain accuracy when the scaling factors themselves must be stored in a low-precision format.",
        "-   **Distributed Systems are Complex**: Features must be designed with multi-GPU execution in mind, often requiring custom communication patterns like the fix for interleaved gathering.",
        "-   **APIs are Abstractions**: The Python `NVFP4Quantizer` class provides a clean interface that hides the immense complexity of the underlying C++/CUDA/distributed logic.",
        "",
        "You are now well-equipped to read through the files in PR #2177, such as `quantize_transpose_vector_blockwise_fp4.cu` and `distributed.py`, and recognize the patterns and algorithms we've discussed here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}